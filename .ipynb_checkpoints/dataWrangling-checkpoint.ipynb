{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext, Window\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import col, lit, signum\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------+--------+---------+-----------+------+\n",
      "|      Date|    Open|    High|     Low|Adj Close|     Volume|Target|\n",
      "+----------+--------+--------+--------+---------+-----------+------+\n",
      "|2000-07-17|4.160714|4.200893|4.080357| 3.605784|  6.50006E7|   0.0|\n",
      "|2000-08-22|3.616071|3.772321|3.598214| 3.196123|  6.92006E7|   2.0|\n",
      "|2000-09-19|4.267857|4.321429|4.183036| 3.706267|  6.78776E7|   0.0|\n",
      "|2000-09-29|2.013393|2.071429|  1.8125| 1.592265|1.8554102E9|   0.0|\n",
      "|2000-10-13|1.446429|1.580357|1.428571| 1.364247| 3.119382E8|   0.0|\n",
      "|2000-11-20|1.328125|1.392857|1.303571|  1.17101| 1.020166E8|   2.0|\n",
      "|2001-01-19|1.388393|1.397321|1.334821| 1.205793|  1.94166E8|   2.0|\n",
      "|2001-03-13|1.348214|1.397321|1.299107| 1.209657| 1.108324E8|   2.0|\n",
      "|2001-04-19|   1.825|1.839286|1.685714|  1.59041| 4.684176E8|   0.0|\n",
      "|2001-06-12|1.412143|1.477857|1.411429|  1.25588|  7.59486E7|   0.0|\n",
      "+----------+--------+--------+--------+---------+-----------+------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      " |-- Volume: double (nullable = true)\n",
      " |-- Target: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Dollar bars\n",
    "df=spark.read.csv(\"AAPL.csv\", header='true')\n",
    "df=df.withColumn('DolExch',col('Adj Close')*col('Volume'))\n",
    "df=df.withColumn('CumDolExch',f.sum(df.DolExch).over(Window.partitionBy().orderBy().rowsBetween(-df.count(),0)))\n",
    "df=df.withColumn('DolBars', col('CumDolExch')%(5E9))\n",
    "df=df.filter((df.DolBars-df.DolExch)<0)\n",
    "\n",
    "\n",
    "#Convert to type double\n",
    "df=df.withColumn(\"Adj Close\",col(\"Adj Close\").cast(\"double\"))\n",
    "df=df.withColumn(\"Volume\",col(\"Volume\").cast(\"double\"))\n",
    "df=df.withColumn(\"Open\",col(\"Open\").cast(\"double\"))\n",
    "df=df.withColumn(\"High\",col(\"High\").cast(\"double\"))\n",
    "df=df.withColumn(\"Low\",col(\"Low\").cast(\"double\"))\n",
    "\n",
    "\n",
    "#Add target vector(2 for +change, 1 for no change 0 for -change)\n",
    "df=df.withColumn(\"next_value\",f.lag(col(\"Adj Close\"),-1).over(Window.orderBy(\"Date\")))\n",
    "df=df.withColumn(\"Target\", 1+signum(df['next_value']-df['Adj Close']))\n",
    "\n",
    "#Drop extra columns, null rows\n",
    "cols_to_drop=['Close','DolBars','DolExch','CumDolExch','next_value']\n",
    "df=df.drop(*cols_to_drop)\n",
    "df=df.na.drop()\n",
    "df.show(10)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "#Create input features\n",
    "df_cols=df.columns\n",
    "df_cols.remove('Date')\n",
    "df_cols.remove('Target')\n",
    "#Move features to a single vector\n",
    "assembler=VectorAssembler(inputCols=df_cols,outputCol=\"features\")\n",
    "df=assembler.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repartitioning\n",
      "run-time: 0.06\n",
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      " |-- Volume: double (nullable = true)\n",
      " |-- Target: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train,test=df.randomSplit([0.8,0.2],seed=1)\n",
    "\n",
    "print('repartitioning')\n",
    "train=train.repartition(10)\n",
    "test=test.repartition(10)\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "dt=DecisionTreeClassifier(featuresCol='features',\n",
    "                         labelCol='Target',\n",
    "                         maxDepth=30,\n",
    "                         minInstancesPerNode=2)\n",
    "\n",
    "dtModel=dt.fit(train)\n",
    "predictions=dtModel.transform(test)\n",
    "end_time=time.time()\n",
    "\n",
    "delta_time = end_time - start_time\n",
    "\n",
    "# 5. print total run time \n",
    "print(f'run-time: {round(delta_time/60.0, 2)}')\n",
    "\n",
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|Target|prediction|\n",
      "+------+----------+\n",
      "|   0.0|       2.0|\n",
      "|   0.0|       2.0|\n",
      "|   2.0|       0.0|\n",
      "|   2.0|       2.0|\n",
      "|   2.0|       0.0|\n",
      "|   2.0|       2.0|\n",
      "|   2.0|       0.0|\n",
      "|   2.0|       2.0|\n",
      "|   2.0|       0.0|\n",
      "|   0.0|       2.0|\n",
      "|   0.0|       2.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       2.0|\n",
      "|   2.0|       2.0|\n",
      "|   2.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   2.0|       0.0|\n",
      "|   2.0|       0.0|\n",
      "|   0.0|       2.0|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "0.5076660988074957\n"
     ]
    }
   ],
   "source": [
    "predictions.select('Target','prediction').show()\n",
    "correct_preds=predictions.filter(col('Target')==col('prediction'))\n",
    "print(correct_preds.count()/predictions.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
