{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import col, signum\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+------+------+----------+-----------------+\n",
      "|Ticker|  Open|  High|   Low| Close|Volume|      Time|__index_level_0__|\n",
      "+------+------+------+------+------+------+----------+-----------------+\n",
      "|    AG| 10.38| 10.38| 10.38| 10.38| 140.0|1566513600|         18051935|\n",
      "|  AVYA|14.715| 14.75|14.705|14.705|1553.0|1591037040|          2235656|\n",
      "|   ALC|51.215|51.215| 51.18| 51.21|1656.0|1586990160|         15178518|\n",
      "|   AXL| 10.02| 10.02| 10.02| 10.02| 219.0|1575679980|          1538479|\n",
      "|  AQUA|  18.8| 18.81|  18.8| 18.81|1191.0|1575936780|          8339089|\n",
      "+------+------+------+------+------+------+----------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Need to carry over from day to day\n",
    "#Dollar bars\n",
    "df=spark.read.parquet(\"A_Candlestick_part.parquet\")\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|           DolExch|\n",
      "+------------------+\n",
      "|             2.057|\n",
      "|            2.3999|\n",
      "|             4.752|\n",
      "|6.9998000000000005|\n",
      "|             12.32|\n",
      "+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+--------+-------+--------+--------+------+----------+-----------------+------------------+------------------+\n",
      "|Ticker|    Open|   High|     Low|   Close|Volume|      Time|__index_level_0__|           DolExch|        CumDolExch|\n",
      "+------+--------+-------+--------+--------+------+----------+-----------------+------------------+------------------+\n",
      "|  ALXN|  125.02| 125.02|  125.01|  125.01| 961.0|1562627400|         14568288|         120134.61|         120134.61|\n",
      "|  ALXN|  125.03| 125.08|  124.95|  125.08|3407.0|1562627460|         14568289|         426147.56|         546282.17|\n",
      "|  ALXN|  125.05| 125.19|  125.05|125.1841|3887.0|1562627520|         14568290|       486590.5967|      1032872.7667|\n",
      "|  ALXN|  125.15| 125.21|  125.07|  125.21|5527.0|1562627580|         14568291| 692035.6699999999|      1724908.4367|\n",
      "|  ALXN|125.2194| 125.28|125.2194|  125.28|1367.0|1562627640|         14568292|         171257.76|      1896166.1967|\n",
      "|  ALXN|  125.28| 125.46|125.2771|  125.36|6635.0|1562627700|         14568293|          831763.6|      2727929.7967|\n",
      "|  ALXN|  125.38| 125.38|  125.27|  125.27|1962.0|1562627760|         14568294|         245779.74|      2973709.5367|\n",
      "|  ALXN|  125.25| 125.25|  125.09|  125.13|6483.0|1562627820|         14568295| 811217.7899999999|      3784927.3267|\n",
      "|  ALXN|  125.17| 125.24|  125.16|  125.18|1804.0|1562627880|         14568296|         225824.72|4010752.0467000003|\n",
      "|  ALXN|  125.11| 125.11|  124.99|  125.03|2001.0|1562627940|         14568297|         250185.03|      4260937.0767|\n",
      "|  ALXN|  125.03| 125.03|  124.91|  124.96|3999.0|1562628000|         14568298|         499715.04|      4760652.1167|\n",
      "|  ALXN|   124.9| 125.02|   124.9|  124.98|6788.0|1562628060|         14568299|         848364.24|      5609016.3567|\n",
      "|  ALXN| 124.995| 125.02|  124.99|  125.01|1312.0|1562628120|         14568300|         164013.12|      5773029.4767|\n",
      "|  ALXN|  124.98|125.005|  124.94|  124.94|2435.0|1562628180|         14568301|          304228.9| 6077258.376700001|\n",
      "|  ALXN|  124.96| 125.11|  124.96|  125.07|3654.0|1562628240|         14568302|457005.77999999997| 6534264.156700001|\n",
      "|  ALXN|  125.07| 125.08|  124.99|  124.99|2832.0|1562628300|         14568303|         353971.68| 6888235.836700001|\n",
      "|  ALXN|  125.06| 125.13| 125.045|  125.13|5246.0|1562628360|         14568304|         656431.98|      7544667.8167|\n",
      "|  ALXN|  125.11| 125.18|  125.07|  125.18|3251.0|1562628420|         14568305|406960.18000000005|      7951627.9967|\n",
      "|  ALXN|  125.19| 125.23|  125.09|  125.11|3722.0|1562628480|         14568306|         465659.42|      8417287.4167|\n",
      "|  ALXN|  125.13|125.225|  125.13| 125.225|2859.0|1562628540|         14568307|358018.27499999997|      8775305.6917|\n",
      "+------+--------+-------+--------+--------+------+----------+-----------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+\n",
      "|          CumDolExch|\n",
      "+--------------------+\n",
      "|           546282.17|\n",
      "|1.2914317311930007E8|\n",
      "| 2.783817081583001E8|\n",
      "|3.2352819185349995E8|\n",
      "|    4.623081662598E8|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----+\n",
      "|Mark|\n",
      "+----+\n",
      "|   1|\n",
      "|   0|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "df=df.withColumn('DolExch',col('Close')*col('Volume'))\n",
    "df.select(\"DolExch\").distinct().show(5)\n",
    "df=df.withColumn(\"CumDolExch\",f.sum(\"DolExch\").over(Window.partitionBy('Ticker').orderBy(\"Time\").rowsBetween(-sys.maxsize,0)))\n",
    "df.show()\n",
    "df.select(\"CumDolExch\").distinct().show(5)\n",
    "df=df.withColumn('DolBars', col('CumDolExch')%(1E7))#Take data pt every 10M dollars exchanged per stock\n",
    "df=df.withColumn('Mark',f.when(df.DolBars<=df.DolExch,1).otherwise(0))\n",
    "df.select(\"Mark\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+--------+--------+------+----------+-----------------+------------------+------------------+------------------+----+-------+\n",
      "|Ticker|    Open|   High|     Low|   Close|Volume|      Time|__index_level_0__|           DolExch|        CumDolExch|           DolBars|Mark|CumMark|\n",
      "+------+--------+-------+--------+--------+------+----------+-----------------+------------------+------------------+------------------+----+-------+\n",
      "|  ALXN|  125.02| 125.02|  125.01|  125.01| 961.0|1562627400|         14568288|         120134.61|         120134.61|         120134.61|   1|      1|\n",
      "|  ALXN|  125.03| 125.08|  124.95|  125.08|3407.0|1562627460|         14568289|         426147.56|         546282.17|         546282.17|   0|      1|\n",
      "|  ALXN|  125.05| 125.19|  125.05|125.1841|3887.0|1562627520|         14568290|       486590.5967|      1032872.7667|      1032872.7667|   0|      1|\n",
      "|  ALXN|  125.15| 125.21|  125.07|  125.21|5527.0|1562627580|         14568291| 692035.6699999999|      1724908.4367|      1724908.4367|   0|      1|\n",
      "|  ALXN|125.2194| 125.28|125.2194|  125.28|1367.0|1562627640|         14568292|         171257.76|      1896166.1967|      1896166.1967|   0|      1|\n",
      "|  ALXN|  125.28| 125.46|125.2771|  125.36|6635.0|1562627700|         14568293|          831763.6|      2727929.7967|      2727929.7967|   0|      1|\n",
      "|  ALXN|  125.38| 125.38|  125.27|  125.27|1962.0|1562627760|         14568294|         245779.74|      2973709.5367|      2973709.5367|   0|      1|\n",
      "|  ALXN|  125.25| 125.25|  125.09|  125.13|6483.0|1562627820|         14568295| 811217.7899999999|      3784927.3267|      3784927.3267|   0|      1|\n",
      "|  ALXN|  125.17| 125.24|  125.16|  125.18|1804.0|1562627880|         14568296|         225824.72|4010752.0467000003|4010752.0467000003|   0|      1|\n",
      "|  ALXN|  125.11| 125.11|  124.99|  125.03|2001.0|1562627940|         14568297|         250185.03|      4260937.0767|      4260937.0767|   0|      1|\n",
      "|  ALXN|  125.03| 125.03|  124.91|  124.96|3999.0|1562628000|         14568298|         499715.04|      4760652.1167|      4760652.1167|   0|      1|\n",
      "|  ALXN|   124.9| 125.02|   124.9|  124.98|6788.0|1562628060|         14568299|         848364.24|      5609016.3567|      5609016.3567|   0|      1|\n",
      "|  ALXN| 124.995| 125.02|  124.99|  125.01|1312.0|1562628120|         14568300|         164013.12|      5773029.4767|      5773029.4767|   0|      1|\n",
      "|  ALXN|  124.98|125.005|  124.94|  124.94|2435.0|1562628180|         14568301|          304228.9| 6077258.376700001| 6077258.376700001|   0|      1|\n",
      "|  ALXN|  124.96| 125.11|  124.96|  125.07|3654.0|1562628240|         14568302|457005.77999999997| 6534264.156700001| 6534264.156700001|   0|      1|\n",
      "|  ALXN|  125.07| 125.08|  124.99|  124.99|2832.0|1562628300|         14568303|         353971.68| 6888235.836700001| 6888235.836700001|   0|      1|\n",
      "|  ALXN|  125.06| 125.13| 125.045|  125.13|5246.0|1562628360|         14568304|         656431.98|      7544667.8167|      7544667.8167|   0|      1|\n",
      "|  ALXN|  125.11| 125.18|  125.07|  125.18|3251.0|1562628420|         14568305|406960.18000000005|      7951627.9967|      7951627.9967|   0|      1|\n",
      "|  ALXN|  125.19| 125.23|  125.09|  125.11|3722.0|1562628480|         14568306|         465659.42|      8417287.4167|      8417287.4167|   0|      1|\n",
      "|  ALXN|  125.13|125.225|  125.13| 125.225|2859.0|1562628540|         14568307|358018.27499999997|      8775305.6917|      8775305.6917|   0|      1|\n",
      "+------+--------+-------+--------+--------+------+----------+-----------------+------------------+------------------+------------------+----+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+\n",
      "|CumMark|\n",
      "+-------+\n",
      "|     26|\n",
      "|     29|\n",
      "|    474|\n",
      "|    964|\n",
      "|   1677|\n",
      "|   1697|\n",
      "|   1806|\n",
      "|   1950|\n",
      "|   2040|\n",
      "|   2214|\n",
      "+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.withColumn('CumMark',f.sum(\"Mark\").over(Window.partitionBy('Ticker').orderBy(\"Time\").rowsBetween(-sys.maxsize,0)))\n",
    "df.show(20)\n",
    "df.select(\"CumMark\").distinct().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------+------+--------+--------+----------+-----------------+------------------+--------------------+------------------+----+-------+\n",
      "|Ticker|    Open|    High|   Low|   Close|  Volume|      Time|__index_level_0__|           DolExch|          CumDolExch|           DolBars|Mark|CumMark|\n",
      "+------+--------+--------+------+--------+--------+----------+-----------------+------------------+--------------------+------------------+----+-------+\n",
      "|  AAPL|199.9402|199.9402|199.86|  199.91| 28682.0|1562628300|         25190662|        5733818.62|     2.70757898436E8| 757898.4359999895|   1|     26|\n",
      "|  AAPL|  199.92|  199.95|199.86|   199.9| 61131.0|1562628360|         25190663| 6486555.100000001|     2.77244453536E8| 7244453.536000013|   0|     26|\n",
      "|  AMZN| 1955.47|  1956.0|199.86|1955.555| 77913.0|1562629500|         11719451|     3.281812401E7|    6.873660628002E8|7366062.8001999855|   1|     26|\n",
      "|  ACIA|    64.9| 64.9485|  64.8|  64.834|133281.0|1562694420|         23123495|       3589728.912|2.7009142026809996E8| 91420.26809996367|   1|     26|\n",
      "|  ACIA|   64.83|   64.83| 64.77|   64.79|188927.0|1562694480|         23123496|3605304.3400000003|2.7369672460809994E8|3696724.6080999374|   0|     26|\n",
      "|  ACIA|   64.79|    64.8| 64.75|  64.765|264436.0|1562694540|         23123497|       4890340.385| 2.785870649930999E8| 8587064.993099928|   0|     26|\n",
      "|  AVGO|  275.14|  275.32| 64.75|  275.32|278078.0|1562697000|          2749963|        3755915.44|    2.802010391862E8| 201039.1862000227|   1|     26|\n",
      "|  AVGO| 275.325|  275.36| 64.75|   275.3|280221.0|1562697060|          2749964|          589967.9|    2.807910070862E8| 791007.0861999989|   0|     26|\n",
      "|  AVGO|  275.31|  275.53| 64.75|  275.47|289443.0|1562697120|          2749965|2540384.3400000003|    2.833313914262E8|3331391.4261999726|   0|     26|\n",
      "|  AVGO|275.4403|  275.75| 64.75|  275.68|299967.0|1562697180|          2749966|2901256.3200000003|2.8623264774619997E8|6232647.7461999655|   0|     26|\n",
      "|  AMAT|   43.71|  43.715|43.675|  43.715|337084.0|1562698500|         14262631|       1622569.655|    2.501151874287E8|115187.42870000005|   1|     26|\n",
      "|  AMAT| 43.7199| 43.7264|43.675|   43.72|356264.0|1562698560|         14262632|          838549.6|    2.509537370287E8| 953737.0286999941|   0|     26|\n",
      "|  AMAT|   43.72|   43.76|43.675|  43.725|373055.0|1562698620|         14262633|        734186.475|    2.516879235037E8|1687923.5036999881|   0|     26|\n",
      "|  AMAT|  43.725|   43.75|43.675|   43.74|395175.0|1562698680|         14262634|          967528.8|    2.526554523037E8|      2655452.3037|   0|     26|\n",
      "|  AMAT|   43.73|   43.73|43.675|  43.715|402128.0|1562698740|         14262635|        303950.395|    2.529594026987E8| 2959402.698700011|   0|     26|\n",
      "|  ADBE| 303.465|  303.66|43.675|303.6405|405523.0|1562698740|         21962085|1030859.4974999999|2.6019443179100004E8|194431.79100003839|   1|     26|\n",
      "|  AMAT|   43.71|   43.73|43.675|   43.69|424259.0|1562698800|         14262636|         818575.84|    2.537779785387E8|3777978.5387000144|   0|     26|\n",
      "|  ADBE|   303.6|   303.7|43.675|  303.58|426333.0|1562698800|         21962086| 629624.9199999999|2.6082405671100003E8| 824056.7110000253|   0|     26|\n",
      "|  AMAT|  43.685|   43.75|43.675|   43.75|454147.0|1562698860|         14262637|         1216862.5|    2.549948410387E8| 4994841.038700014|   0|     26|\n",
      "|  ADBE| 303.555|  303.61|43.675|  303.49|459448.0|1562698860|         21962087|        1608800.49|2.6243285720100003E8| 2432857.201000035|   0|     26|\n",
      "+------+--------+--------+------+--------+--------+----------+-----------------+------------------+--------------------+------------------+----+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Row(Ticker='AAPL', Open=199.9402, High=199.9402, Low=199.86, Close=199.91, Volume=28682.0, Time=1562628300, __index_level_0__=25190662, DolExch=5733818.62, CumDolExch=270757898.436, DolBars=757898.4359999895, Mark=1, CumMark=26)\n",
      "Row(Ticker='AAPL', Open=199.9402, High=199.9402, Low=199.86, Close=199.91, Volume=28682.0, Time=1562628300, __index_level_0__=25190662, DolExch=5733818.62, CumDolExch=270757898.436, DolBars=757898.4359999895, Mark=1, CumMark=26)\n",
      "Row(Ticker='ALXN', Open=47.61, High=125.02, Low=1.26, Close=125.01, Volume=3461073.0, Time=1562627400, __index_level_0__=14568288, DolExch=120134.61, CumDolExch=120134.61, DolBars=120134.61, Mark=1, CumMark=1, next_val=125.18, Target=0.0013598912087033172)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(Ticker='ALXN', Time=1562627400, Open=47.61, Low=1.26, High=125.02, Close=125.01, Volume=3461073.0, Target=0.0013598912087033172)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.withColumn(\"Volume\", f.sum(\"Volume\").over(Window.partitionBy(\"CumMark\").orderBy(\"Time\").rowsBetween(-sys.maxsize,0)))\n",
    "\n",
    "df=df.withColumn(\"Low\",f.min(\"Low\").over(Window.partitionBy(\"CumMark\").orderBy(\"Time\").rowsBetween(-sys.maxsize,0)))\n",
    "df.show()\n",
    "print(df.head())\n",
    "df=df.withColumn(\"Open\",f.first(\"Open\").over(Window.partitionBy(\"CumMark\").orderBy(\"Time\").rowsBetween(-sys.maxsize,0)))\n",
    "print(df.head())\n",
    "df=df.withColumn(\"Close\",f.last(\"Close\").over(Window.partitionBy(\"CumMark\").orderBy(\"Time\").rowsBetween(-sys.maxsize,0)))\n",
    "df=df.filter(df.Mark==1)\n",
    "\n",
    "#Add target vector(2 for +change, 1 for no change 0 for -change)\n",
    "df=df.withColumn(\"next_val\", f.lead(col(\"Close\"),1).over(Window.partitionBy('Ticker').orderBy(df[\"Time\"])))#Need to add partitionBy()\n",
    "df=df.withColumn(\"Target\", (col(\"next_val\")-col(\"Close\"))/col(\"Close\"))\n",
    "print(df.head())\n",
    "df=df.select(\"Ticker\",\"Time\",\"Open\",\"Low\",\"High\",\"Close\",\"Volume\",\"Target\")#.withColumn(\"Target\", 100*(col(\"next_val\")-col(\"Close\"))/col(\"Close\"))\n",
    "#df=df.drop(\"next_val\").withColumn(\"Target\",1+signum(col(\"Target\")))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ALXN': '1', 'AWAY': '2', 'AAT': '3', 'ABMD': '4', 'AEFC': '5', 'AESR': '6', 'AIV': '7', 'APM': '8', 'ARYAW': '9', 'AVY': '10', 'ARAY': '11', 'AMTX': '12', 'ARL': '13', 'AXP': '14', 'ACCO': '15', 'ARTW': '16', 'ADIL': '17', 'AVNW': '18', 'AVUV': '19', 'ALDX': '20', 'APYX': '21', 'ASH': '22', 'ARGT': '23', 'ATCX': '24', 'AHCO': '25', 'AMRHW': '26', 'AMRK': '27', 'AVEO': '28', 'AVXL': '29', 'AXU': '30', 'AEB': '31', 'ALE': '32', 'ALSN': '33', 'ASYS': '34', 'AVDL': '35', 'ACH': '36', 'AGIO': '37', 'ARA': '38', 'AQMS': '39', 'ASET': '40', 'ALG': '41', 'ASHX': '42', 'AVAL': '43', 'ALKS': '44', 'ACWX': '45', 'AEY': '46', 'AM': '47', 'AOM': '48', 'ARCT': '49', 'AYX': '50', 'AA': '51', 'ACHC': '52', 'AMRH': '53', 'ASIX': '54', 'AAPL': '55', 'AMPH': '56', 'ANDE': '57', 'AY': '58', 'ADNT': '59', 'ALL': '60', 'AOSL': '61', 'ADVM': '62', 'AMRX': '63', 'ACGLO': '64', 'ADI': '65', 'AFGC': '66', 'ALACU': '67', 'ARB': '68', 'AZZ': '69', 'AGNC': '70', 'ALTR': '71', 'AGMH': '72', 'APDN': '73', 'AEF': '74', 'ARKK': '75', 'AROC': '76', 'ARKG': '77', 'ARPO': '78', 'AAMC': '79', 'ACA': '80', 'AMBA': '81', 'APXT': '82', 'ARC': '83', 'AGGY': '84', 'AFI': '85', 'AFIN': '86', 'AMCA': '87', 'AMGN': '88', 'AIRT': '89', 'ANTM': '90', 'ARVN': '91', 'ABM': '92', 'ARTL': '93', 'ACGLP': '94', 'AEIS': '95', 'AGRX': '96', 'AKBA': '97', 'AMK': '98', 'AMTI': '99', 'AOS': '100', 'ATHM': '101', 'ADPT': '102', 'AMU': '103', 'AT': '104', 'ATRS': '105', 'AVCTW': '106', 'AMEH': '107', 'ADUS': '108', 'AER': '109', 'AIRG': '110', 'ANDAW': '111', 'ADX': '112', 'AFGH': '113', 'AACG': '114', 'AGI': '115', 'ALJJ': '116', 'AFYA': '117', 'AIR': '118', 'ATOS': '119', 'ABEO': '120', 'ADAP': '121', 'ALX': '122', 'AMBO': '123', 'AVTR': '124', 'AZPN': '125', 'ACC': '126', 'AGD': '127', 'APO': '128', 'ATRI': '129', 'AIKI': '130', 'ATAX': '131', 'ACN': '132', 'AMED': '133', 'ANIP': '134', 'ARE': '135', 'ABTX': '136', 'AIG': '137', 'AFL': '138', 'ANAB': '139', 'APPS': '140', 'AGNCO': '141', 'APH': '142', 'ATRA': '143', 'ANGI': '144', 'ASTE': '145', 'AMOM': '146', 'ANCN': '147', 'ABIO': '148', 'ADILW': '149', 'AGFSW': '150', 'AGGP': '151', 'ARTLW': '152', 'ASFI': '153', 'ALIM': '154', 'AMNB': '155', 'ALLY': '156', 'AME': '157', 'ANSS': '158', 'ASR': '159', 'AVDV': '160', 'ABEV': '161', 'ALLT': '162', 'APPF': '163', 'ARCH': '164', 'ARMR': '165', 'AFTY': '166', 'AKER': '167', 'ANTE': '168', 'ASPU': '169', 'ABBV': '170', 'ACNB': '171', 'ACT': '172', 'AQST': '173', 'ARCE': '174', 'ATH': '175', 'AXTA': '176', 'ACHV': '177', 'ARDX': '178', 'AUB': '179', 'ADP': '180', 'AAP': '181', 'AFC': '182', 'ALGN': '183', 'AMCI': '184', 'ARGX': '185', 'AXGN': '186', 'ADM': '187', 'AIRR': '188', 'AFG': '189', 'AFINP': '190', 'AKRO': '191', 'AMSF': '192', 'AJXA': '193', 'AMT': '194', 'ACES': '195', 'ASC': '196', 'AU': '197', 'AUTO': '198', 'ALC': '199', 'APRE': '200', 'AUMN': '201', 'AZAA': '202', 'ALAC': '203', 'ATEX': '204', 'ATMP': '205', 'ALEX': '206', 'AAON': '207', 'ALLK': '208', 'AZBA': '209', 'ACIA': '210', 'AFGD': '211', 'ARGD': '212', 'AVB': '213', 'AXO': '214', 'AIZ': '215', 'ALLO': '216', 'APHA': '217', 'AUSF': '218', 'AZN': '219', 'AHC': '220', 'AMRB': '221', 'ANGO': '222', 'ARKW': '223', 'ARES': '224', 'ASRT': '225', 'AAWW': '226', 'ALYA': '227', 'AUTL': '228', 'ABUS': '229', 'AES': '230', 'AQB': '231', 'ATRC': '232', 'ASGN': '233', 'ASNA': '234', 'AYLA': '235', 'AVT': '236', 'AGR': '237', 'AIIQ': '238', 'AIN': '239', 'ANDA': '240', 'APVO': '241', 'ACM': '242', 'AEM': '243', 'AMS': '244', 'AOR': '245', 'AQUA': '246', 'ATKR': '247', 'ACER': '248', 'AXSM': '249', 'AMRS': '250', 'AMTB': '251', 'APLT': '252', 'ARNA': '253', 'APEI': '254', 'ABG': '255', 'ACLS': '256', 'ARDC': '257', 'AAXN': '258', 'AKO.A': '259', 'APOP': '260', 'AVLR': '261', 'APRN': '262', 'ASHS': '263', 'AXL': '264', 'ACAM': '265', 'AI': '266', 'AIRTP': '267', 'AKR': '268', 'ALFA': '269', 'AVCO': '270', 'AGCO': '271', 'ALB': '272', 'AUBN': '273', 'ADRO': '274', 'AGRO': '275', 'ARI': '276', 'AKCA': '277', 'APTV': '278', 'ARMP': '279', 'ANF': '280', 'AVD': '281', 'AXAS': '282', 'ABCB': '283', 'ADMP': '284', 'AMRC': '285', 'AMTD': '286', 'APPN': '287', 'ACWV': '288', 'AGTC': '289', 'ARYBU': '290', 'ATHE': '291', 'AMAL': '292', 'ASPS': '293', 'AL': '294', 'APTX': '295', 'ATVI': '296', 'AYI': '297', 'APEX': '298', 'ARLO': '299', 'ATV': '300', 'AEP': '301', 'AEYE': '302', 'ARKR': '303', 'AGYS': '304', 'ATLC': '305', 'AWK': '306', 'AWTM': '307', 'ALPN': '308', 'APG': '309', 'ADS': '310', 'AEGN': '311', 'ASRV': '312', 'AFGB': '313', 'AGEN': '314', 'ATHX': '315', 'AGG': '316', 'AAOI': '317', 'ACST': '318', 'ACWF': '319', 'AHT': '320', 'AUPH': '321', 'A': '322', 'ACV': '323', 'AGX': '324', 'ANIK': '325', 'AMUB': '326', 'AOK': '327', 'ABEQ': '328', 'ALBO': '329', 'AIMT': '330', 'ALV': '331', 'ANPC': '332', 'AON': '333', 'AMC': '334', 'ANDAU': '335', 'AUG': '336', 'AKO.B': '337', 'ARQT': '338', 'ATO': '339', 'AWR': '340', 'ATCXW': '341', 'ADSK': '342', 'ANDAR': '343', 'ANIX': '344', 'ARCB': '345', 'ATEC': '346', 'AAAU': '347', 'AKTX': '348', 'AAL': '349', 'AESE': '350', 'AMOV': '351', 'AWF': '352', 'ALT': '353', 'AMLP': '354', 'AWP': '355', 'ABR': '356', 'ACMR': '357', 'AIH': '358', 'ANVS': '359', 'AOA': '360', 'APA': '361', 'AAXJ': '362', 'APLS': '363', 'AGNCM': '364', 'AVUS': '365', 'AFMD': '366', 'AMTBB': '367', 'APEN': '368', 'APWC': '369', 'ATR': '370', 'AAN': '371', 'ATEN': '372', 'ATOM': '373', 'AVGO': '374', 'AMCX': '375', 'APAM': '376', 'ATGE': '377', 'AVID': '378', 'ACU': '379', 'AMAT': '380', 'ARCC': '381', 'ASB': '382', 'ACRE': '383', 'AGLE': '384', 'AIT': '385', 'AMP': '386', 'AYRO': '387', 'APTS': '388', 'AROW': '389', 'ALTG': '390', 'AMD': '391', 'AMN': '392', 'ARGO': '393', 'ACRS': '394', 'AGZD': '395', 'AJX': '396', 'ARYAU': '397', 'AXNX': '398', 'AB': '399', 'ACIU': '400', 'AFMC': '401', 'ANH': '402', 'ARAV': '403', 'AFT': '404', 'ADC': '405', 'ADES': '406', 'AMG': '407', 'ASM': '408', 'ACTG': '409', 'ACWI': '410', 'AEO': '411', 'ASRVP': '412', 'AWRE': '413', 'ARCM': '414', 'ARW': '415', 'ACOR': '416', 'AGNCN': '417', 'ASEA': '418', 'AGM-A': '419', 'ANY': '420', 'APLE': '421', 'ACIO': '422', 'AEHR': '423', 'AIA': '424', 'AAU': '425', 'AEL': '426', 'AMPE': '427', 'APOPW': '428', 'ARR': '429', 'ACRX': '430', 'AMH': '431', 'ABT': '432', 'AIMC': '433', 'AINV': '434', 'AIRTW': '435', 'AQNB': '436', 'AXS': '437', 'AZEK': '438', 'ARNC': '439', 'ARYA': '440', 'ADMA': '441', 'AMKR': '442', 'AMWD': '443', 'AWI': '444', 'AZUL': '445', 'AMX': '446', 'ATNI': '447', 'ALNA': '448', 'AMAG': '449', 'APOG': '450', 'ADME': '451', 'AINC': '452', 'AIZP': '453', 'ALNY': '454', 'AOD': '455', 'ATI': '456', 'AJG': '457', 'AMOT': '458', 'AZRX': '459', 'AIEQ': '460', 'ATRO': '461', 'ABB': '462', 'ACSG': '463', 'AREC': '464', 'ASA': '465', 'AGO': '466', 'AXDX': '467', 'ARDS': '468', 'ATTO': '469', 'AFSM': '470', 'ALACR': '471', 'ALGT': '472', 'AMSC': '473', 'ASLN': '474', 'AVYA': '475', 'AKTS': '476', 'ALLE': '477', 'ARKF': '478', 'ASMB': '479', 'AERI': '480', 'ALSK': '481', 'APD': '482', 'ARLP': '483', 'ACIW': '484', 'ADCT': '485', 'AMSWA': '486', 'ATIF': '487', 'AVCT': '488', 'AIC': '489', 'AIRI': '490', 'ASUR': '491', 'ATNM': '492', 'AYTU': '493', 'AFK': '494', 'AMZA': '495', 'AWH': '496', 'ALEC': '497', 'ARWR': '498', 'ASHR': '499', 'AXR': '500', 'AFB': '501', 'AFIF': '502', 'AKAM': '503', 'AZRE': '504', 'ALCO': '505', 'ASX': '506', 'AMHC': '507', 'AUDC': '508', 'ABC': '509', 'AC': '510', 'AIHS': '511', 'ALTM': '512', 'AMBC': '513', 'ASTC': '514', 'AN': '515', 'ARD': '516', 'ATCO': '517', 'AVNS': '518', 'ACBI': '519', 'ACY': '520', 'ANAT': '521', 'AQNA': '522', 'ATNX': '523', 'ADT': '524', 'ADXS': '525', 'AGFS': '526', 'ARMK': '527', 'AZO': '528', 'ASND': '529', 'AUY': '530', 'AVRO': '531', 'ALTS': '532', 'AMPY': '533', 'ARTNA': '534', 'ATLO': '535', 'AEE': '536', 'ADTN': '537', 'AGM': '538', 'AIO': '539', 'ALK': '540', 'ALO': '541', 'AMRN': '542', 'AMZN': '543', 'AXLA': '544', 'ATXI': '545', 'AXGT': '546', 'AGZ': '547', 'AHH': '548', 'ALRS': '549', 'AXTI': '550', 'AJRD': '551', 'ADSW': '552', 'ADBE': '553', 'AMCR': '554', 'AVAV': '555', 'AR': '556', 'ATSG': '557', 'AVDE': '558', 'AGE': '559', 'ANET': '560', 'ACAD': '561', 'ACAMU': '562', 'AEG': '563', 'ARKQ': '564', 'ACB': '565', 'AMHCW': '566', 'ANGL': '567', 'AADR': '568', 'AGS': '569', 'AMJ': '570', 'AP': '571', 'APTO': '572', 'AFLG': '573', 'AIF': '574', 'AIM': '575', 'AQN': '576', 'AVA': '577', 'AVEM': '578', 'AGNCP': '579', 'AMCIW': '580', 'APT': '581', 'ASG': '582', 'AUBAP': '583', 'AIQ': '584', 'ASML': '585', 'AWX': '586', 'AFH': '587', 'ACAMW': '588', 'ACGL': '589', 'ALRM': '590', 'ALRN': '591', 'APXTU': '592', 'AIW': '593', 'AMCIU': '594', 'APXTW': '595', 'ASPN': '596', 'AEMD': '597', 'ACP': '598', 'AGT': '599', 'AMHCU': '600', 'ADMS': '601', 'ALOT': '602', 'AGQ': '603', 'AHPI': '604', 'ALTY': '605', 'ALUS': '606', 'ARCO': '607', 'AX': '608', 'ACEL': '609', 'ATUS': '610', 'AVGR': '611'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ticker: string (nullable = true)\n",
      " |-- Time: long (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: double (nullable = true)\n",
      " |-- Target: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Ticker: integer (nullable = true)\n",
      " |-- Time: long (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: double (nullable = true)\n",
      " |-- Target: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Convert Ticker value to int to include in ML algo, dictionary present (tick_dict) to convert back if needed\n",
    "tickers=df.select(\"Ticker\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "tick_dict = {val : str(idx + 1) for idx, val in enumerate(tickers)} \n",
    "print(tick_dict)\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "df=df.replace(to_replace=tick_dict, subset=['Ticker'])\n",
    "\n",
    "df.printSchema()\n",
    "df=df.withColumn(\"Ticker\",col(\"Ticker\").cast(IntegerType()))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "583540\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "#Create input features\n",
    "df_cols=df.columns\n",
    "df_cols=[ elem for elem in df_cols if elem not in [\"Time\",\"Target\"]]\n",
    "#Move features to a single vector\n",
    "assembler=VectorAssembler(inputCols=df_cols,outputCol=\"features\")\n",
    "df=assembler.transform(df)\n",
    "df=df.na.drop()\n",
    "df=df.dropDuplicates()\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+------+--------+--------+---------+--------------------+--------------------+\n",
      "|Ticker|      Time|     Open|   Low|    High|   Close|   Volume|              Target|            features|\n",
      "+------+----------+---------+------+--------+--------+---------+--------------------+--------------------+\n",
      "|     1|1562693400|   199.95| 21.72|   124.5|  123.68|1327968.0|0.011238680465717871|[1.0,199.95,21.72...|\n",
      "|     1|1562880300|  199.935| 21.51|  120.75|   120.7|2224125.0|-2.48550124275071...|[1.0,199.935,21.5...|\n",
      "|     1|1564517880|  1989.28| 49.58|  114.81|  114.76| 831781.0|-0.00660073196235...|[1.0,1989.28,49.5...|\n",
      "|     1|1564703700|  1989.35| 50.48|  114.85|  114.64| 784528.0|9.595254710397718E-4|[1.0,1989.35,50.4...|\n",
      "|     1|1565391180|2013.0727|46.705|   109.2| 109.155|1044949.0|0.007558059639961549|[1.0,2013.0727,46...|\n",
      "|     1|1566510540|  2021.53| 47.72|   123.4|  122.06|1063452.0|0.003604784532197...|[1.0,2021.53,47.7...|\n",
      "|     1|1566841260|  2020.81|45.255|  110.81|110.7325|1024557.0|-0.00300273180863...|[1.0,2020.81,45.2...|\n",
      "|     1|1570038900|2011.4897| 47.84|   95.53|  95.385|1119747.0|0.015883000471772296|[1.0,2011.4897,47...|\n",
      "|     1|1574474040|2011.3471| 49.97|   110.9|  110.89|1190974.0|0.004433222112002934|[1.0,2011.3471,49...|\n",
      "|     1|1576893600|  1997.64| 52.81|109.7525|  109.75|1619781.0|                 0.0|[1.0,1997.64,52.8...|\n",
      "+------+----------+---------+------+--------+--------+---------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)\n",
    "df=df.na.drop()\n",
    "df=df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+------+--------+--------+---------+--------------------+--------------------+\n",
      "|Ticker|      Time|     Open|   Low|    High|   Close|   Volume|              Target|            features|\n",
      "+------+----------+---------+------+--------+--------+---------+--------------------+--------------------+\n",
      "|     1|1562693400|   199.95| 21.72|   124.5|  123.68|1327968.0|0.011238680465717871|[1.0,199.95,21.72...|\n",
      "|     1|1562880300|  199.935| 21.51|  120.75|   120.7|2224125.0|-2.48550124275071...|[1.0,199.935,21.5...|\n",
      "|     1|1564517880|  1989.28| 49.58|  114.81|  114.76| 831781.0|-0.00660073196235...|[1.0,1989.28,49.5...|\n",
      "|     1|1564703700|  1989.35| 50.48|  114.85|  114.64| 784528.0|9.595254710397718E-4|[1.0,1989.35,50.4...|\n",
      "|     1|1566510540|  2021.53| 47.72|   123.4|  122.06|1063452.0|0.003604784532197...|[1.0,2021.53,47.7...|\n",
      "|     1|1566841260|  2020.81|45.255|  110.81|110.7325|1024557.0|-0.00300273180863...|[1.0,2020.81,45.2...|\n",
      "|     1|1570038900|2011.4897| 47.84|   95.53|  95.385|1119747.0|0.015883000471772296|[1.0,2011.4897,47...|\n",
      "|     1|1574474040|2011.3471| 49.97|   110.9|  110.89|1190974.0|0.004433222112002934|[1.0,2011.3471,49...|\n",
      "|     1|1576893600|  1997.64| 52.81|109.7525|  109.75|1619781.0|                 0.0|[1.0,1997.64,52.8...|\n",
      "|     1|1581039540|   1983.3| 27.08|  104.15|  104.14|1907343.0|-0.00124831956980...|[1.0,1983.3,27.08...|\n",
      "+------+----------+---------+------+--------+--------+---------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "repartitioning\n",
      "End repartition\n",
      "Start training\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression  import RandomForestRegressor\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train,test=df.randomSplit([0.8,0.2],seed=1)\n",
    "train.show(10)\n",
    "print('repartitioning')\n",
    "train=train.repartition(100)\n",
    "test=test.repartition(100)\n",
    "print('End repartition')\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "dt=RandomForestRegressor(featuresCol='features',\n",
    "                         labelCol='Target',\n",
    "                         maxDepth=30,\n",
    "                         minInstancesPerNode=2)\n",
    "print('Start training')\n",
    "dtModel=dt.fit(train)\n",
    "print('End training')\n",
    "predictions=dtModel.transform(test)\n",
    "end_time=time.time()\n",
    "\n",
    "delta_time = end_time - start_time\n",
    "\n",
    "# 5. print total run time \n",
    "print(f'run-time: {round(delta_time, 2)}')\n",
    "\n",
    "predictions.printSchema()\n",
    "\n",
    "#predictions.select('Time','Target','probability','prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.ml.tuning import CrossValidator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max=predictions.agg({\"prediction\":\"max\"}).collect()[0]\n",
    "max=max[\"max(prediction)\"]#Max predicted increase\n",
    "\n",
    "toDisplay=predictions.select(\"Time\",\"Target\",\"prediction\")\n",
    "toDisplay=toDisplay.withColumn(\"ToInvest\",f.when(col('prediction')>0,1000.*col(\"prediction\")/max).otherwise(0))\n",
    "toDisplay=toDisplay.withColumn(\"Revenue\", col(\"ToInvest\")*col(\"Target\"))\n",
    "#toDisplay=toDisplay.withColumn(\"Target\",f.round(100*col(\"Target\"),5))\n",
    "#toDisplay=toDisplay.withColumn(\"prediction\",f.round(100*col(\"prediction\"),5))\n",
    "toDisplay.show()\n",
    "print(predictions.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profit_row=toDisplay.agg({\"Revenue\":\"sum\"}).collect()[0]\n",
    "\n",
    "profit=profit_row[\"sum(Revenue)\"]\n",
    "print(profit)\n",
    "\n",
    "toDisplay.filter(col(\"ToInvest\")>0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.repartition(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o43.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:829)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 24, 192.168.0.5, executor driver): java.io.FileNotFoundException: File file:/Users/grantjensen/Backtesting/A_Candlestick.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:490)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:275)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n\t... 33 more\nCaused by: java.io.FileNotFoundException: File file:/Users/grantjensen/Backtesting/A_Candlestick.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:490)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:275)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9d8acaa40359>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"A_Candlestick\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/share/virtualenvs/grantjensen-LUZRbGIG/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/grantjensen-LUZRbGIG/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/grantjensen-LUZRbGIG/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/grantjensen-LUZRbGIG/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o43.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:829)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 24, 192.168.0.5, executor driver): java.io.FileNotFoundException: File file:/Users/grantjensen/Backtesting/A_Candlestick.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:490)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:275)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n\t... 33 more\nCaused by: java.io.FileNotFoundException: File file:/Users/grantjensen/Backtesting/A_Candlestick.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:490)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:275)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df.write.parquet(\"A_Candlestick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o42.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 13.0 failed 1 times, most recent failure: Lost task 0.0 in stage 13.0 (TID 26, 192.168.0.5, executor driver): java.io.FileNotFoundException: File file:/Users/grantjensen/Backtesting/A_Candlestick.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:490)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:275)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:467)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3625)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2695)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2695)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2902)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\nCaused by: java.io.FileNotFoundException: File file:/Users/grantjensen/Backtesting/A_Candlestick.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:490)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:275)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-eb589bae8d4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/share/virtualenvs/grantjensen-LUZRbGIG/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \"\"\"\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/grantjensen-LUZRbGIG/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/grantjensen-LUZRbGIG/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/grantjensen-LUZRbGIG/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o42.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 13.0 failed 1 times, most recent failure: Lost task 0.0 in stage 13.0 (TID 26, 192.168.0.5, executor driver): java.io.FileNotFoundException: File file:/Users/grantjensen/Backtesting/A_Candlestick.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:490)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:275)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:467)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3625)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2695)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2695)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2902)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\nCaused by: java.io.FileNotFoundException: File file:/Users/grantjensen/Backtesting/A_Candlestick.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:490)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:275)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
